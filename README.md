# zero-shot-classifiers-for-conceptual-engineering
 Experiments with conceptual engineering using an LLM-based implementation of Nado's classification procedures

## Overview

This repository contains code for using large language models to implement targets of conceptual engineering, and demonstrates it using data from Wikidata to evaluate concept definitions from two paradigmatic conceptual engineering projects: the International Astronomical Union's redefinition of PLANET and Haslanger's ameliorative analysis of WOMAN.

## Installation

## License
MIT.

## Background

*Conceptual engineering* (CE) is a philosophical methodology concerned with the assessment and improvement of concepts \cite{cappelen2018fixing}. Koch, Löhr and Pinder have surveyed recent work on the theory of CE, discussing different theories defining the targets of CE, i.e., "\textit{what} conceptual engineers are (or should be) trying to engineer" \cite{koch2023recent}. In one such theory, Nado proposes as targets \textit{classification procedures}, defined as abstract 'recipes' which sort entities "into an 'in'-group and an 'out'-group" \cite{nado2021classification}. Our work builds on Nado's idea by defining a method for implementing classification procedures consistent with this definition. 

A *large language model* (LLM) is a probabilistic model trained on a natural language corpus that, given a sequence of tokens from a vocabulary occurring in the corpus, generates a continuation of the input sequence. LLMs exhibit remarkable capabilities for natural language processing and generation \cite{brown2020language}. Our work uses \textit{prompt engineering} \cite{liu2023pre} of LLMs to implement classification procedures. 

A *knowledge graph* represents knowledge using nodes for entities and edges for relations \cite{10.1145/3447772}. Knowledge graphs are key information infrastructure for many Web applications \cite{heist2020knowledge}. Our work leverages knowledge graphs as a source of entities used to evaluate classification procedures.

## Implementation
Figure 1 illustrates our method for implementing classification procedures as zero-shot chain-of-thought \cite{kojima2022large} classifiers. Given a concept's name and intensional definition and an entity's name and description, we prompt an LLM to generate a rationale arguing for or against the entity as an element of the concept's extension, followed by a final 'positive' or 'negative' answer.

\begin{figure*}
    \includegraphics[width=\columnwidth]{./images/classifier_example.png}
    \caption{A classification procedure using the 24 August 2006 version of the IAU definition of PLANET, implemented as a zero-shot chain-of-thought classifier, and being applied to the description of the entity DENIS-P J08230313-491201 b.}
    \label{fig:classifier_example}
\end{figure*}

### classification_procedure.py

## Experiments
To evaluate classification procedures built using this method, we sample positive and negative examples of a concept from the Wikidata collaborative knowledge graph \cite{vrandevcic2014wikidata}, retrieving for each entity a summary of its Wikipedia page to use as its description. Next, we apply the classification procedure for a given definition of the concept to each example and compute a confusion matrix from the classifications, which provides performance metrics for the classification procedure. False positives/negatives are then reviewed to determine if a given error arises from the concept's definition or the entity's description.

All definitions are used verbatim. For the LLM, we use GPT-4 \cite{openai2023gpt4} with a temperature setting of 0.1.

### planet_experiment.ipynb
We evaluated three definitions for PLANET: one from the Oxford English Dictionary (OED) \cite{oedplanet} and two from the 2006 International Astronomical Union (IAU) General Assembly \cite{iauplanetdraft2006,assembly2006result}). We sampled 50 positive examples that are instances (P31) of planet (Q634), and 50 negative examples that are instances of substellar object (Q3132741), but not of planet. 

### woman_experiment.ipynb
We also evaluated three definitions for WOMAN: one from the OED \cite{oedwoman}, the definition provided in Haslanger’s 2000 paper \cite{haslanger2000gender}, and one from the Homosaurus vocabulary of LGBTQ+ terms \cite{homosauruswomen,cifor2022mediating}. We sampled 50 positive examples whose sex or gender (P21) is either female (Q6581072) or trans woman (Q1052281), and 50 negative examples whose sex or gender is either male (Q6581097), non-binary (Q48270), or trans man (Q2449503).

## Findings
Rationales generated by the classification procedures were sound, and answers were faithful to their rationales. Rationales frequently contained statements about issues with concept definitions or entity descriptions. For example. the rationale in Figure \ref{fig:classifier_example} correctly notes a problem with the given IAU definition's use of the term "Sun" instead of e.g. "star" \cite{sarma2008iau}, while also noting the lack of discriminating details in the description of DENIS-P J08230313-491201 b (Q17010263).

Table \ref{tab:summary_performance_metrics} provides a summary of the performance metrics from the experiments. For PLANET, all three classification procedures performed well, with the final (24 August 2006) IAU definition performing best. The majority of errors were false positives relating to trans-Neptunian objects, the classification of which was a motivation for the IAU redefinition of PLANET. All of the classification procedures had 2MASS J03552337+1133437 (Q222246) as a false negative, which was rejected due to its identification as a brown dwarf. For WOMAN, all three classification procedures also performed well. The Homosaurus definition performed best, possibly because it is the most inclusive definition. Haslanger's definition performed slightly worse, with two false negatives due to entity descriptions lacking evidence of systematic subordination. The Wikidata community is working to improve the modeling of gender in Wikidata \cite{wikidatagender2023}, evidenced by the observed alignment with inclusive definitions.

## Discussion
We claim the above approach shows how a CE project can incorporate an empirical, data-driven activity \cite{andow2020fully}. Applying classification procedures to large numbers of positive and negative examples of elements of a concept's extension can help conceptual engineers evaluate different definitions for a concept at a scale that "armchair-based conceptual engineering" \cite{landesconceptual} cannot. Rationales generated by classification procedures can help conceptual engineers refine their definitions. This raises the possibility that generative AI assistants \cite{weisz2023toward} could support philosophers in the conduct of CE projects.

Knowledge graphs such as Wikidata have an impact on society by virtue of their use in online search and recommendation \cite{peng2023knowledge}. Using classification procedures to evaluate and improve the alignment between natural language definitions of concepts and the representation of their extensions in knowledge graphs can be of practical value in knowledge graph refinement \cite{paulheim2017knowledge}, socially responsible data management \cite{stoyanovich2022responsible}, and data governance \cite{khatri2010designing}. We believe this provides a new perspective on success conditions for CE \cite{andow2021conceptual,pinder2022haslanger}, leading us to the ameliorative refinement \cite{podosky2022can} of knowledge graphs as a topic for future research.

A limitation of our work is its reliance on a closed API, which raises transparency, reproducibility and safety concerns \cite{bender2021dangers, hu2023prompting}. Further work is needed to evaluate our method with respect to these issues, with a specific focus on evaluating explanation faithfulness \cite{turpin2023language}.

## References

